#Kaggle Titanic: Machine Learning from Disaster
*757 RUG - Submission 2*

-------

The first Kaggle model submitted was extremely basic and simply predicted all died. The result was 62.679% accuracy.

This submission is likely not going to get a better accuracy, but is meant to test code modification in Git. The algorithm randomly selects a proportion of the test set equal to that observed in the training set.

So what was the result...only 57.895% accurate.  This supports the phenomenon demonstrated in Submission 1 and described in machine learning classes. When the data has a preponderance of a particular attribute, simply guessing all as an output will likely beat any prediction. That's a lesson in Occam's Razor, eh?

-Jay